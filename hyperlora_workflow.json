{
  "3": {
    "class_type": "VAELoader",
    "inputs": {
      "vae_name": "sdxl_vae.safetensors"
    }
  },
  "10": {
    "class_type": "CheckpointLoaderSimple",
    "inputs": {
      "ckpt_name": "realvis.safetensors"
    }
  },
  "15": {
    "class_type": "CLIPSetLastLayer",
    "inputs": {
      "clip": [
        "10",
        1
      ],
      "stop_at_clip_layer": -2
    }
  },
  "4": {
    "class_type": "CLIPTextEncode",
    "inputs": {
      "text": "fcsks fxhks fhyks, {prompt}",
      "clip": [
        "15",
        0
      ]
    }
  },
  "5": {
    "class_type": "CLIPTextEncode",
    "inputs": {
      "text": "{negative_prompt}",
      "clip": [
        "15",
        0
      ]
    }
  },
  "20": {
    "class_type": "LoadImage",
    "inputs": {
      "image": "source.png"
    }
  },
  "6": {
    "class_type": "EmptyLatentImage",
    "inputs": {
      "width": 768,
      "height": 1024,
      "batch_size": 1
    }
  },
  "21": {
    "class_type": "APersonMaskGenerator",
    "inputs": {
      "images": [
        "20",
        0
      ],
      "face_mask": true,
      "background_mask": false,
      "hair_mask": true,
      "body_mask": false,
      "clothes_mask": false,
      "confidence": 0.4,
      "refine_mask": false
    }
  },
  "22": {
    "class_type": "GrowMask",
    "inputs": {
      "mask": [
        "21",
        0
      ],
      "expand": 10,
      "tapered_corners": true
    }
  },
  "23": {
    "class_type": "VAEEncode",
    "inputs": {
      "pixels": [
        "20",
        0
      ],
      "vae": [
        "3",
        0
      ]
    }
  },
  "24": {
    "class_type": "SetLatentNoiseMask",
    "inputs": {
      "samples": [
        "23",
        0
      ],
      "mask": [
        "22",
        0
      ]
    }
  },
  "2": {
    "class_type": "HyperLoRAConfig",
    "inputs": {
      "image_processor": "clip_vit_large_14_processor",
      "image_encoder": "clip_vit_large_14",
      "resampler.dim": 1024,
      "resampler.dim_head": 64,
      "resampler.heads": 12,
      "resampler.depth": 4,
      "resampler.ff_mult": 4,
      "encoder_types": "clip + arcface",
      "face_analyzer": "antelopev2",
      "id_embed_dim": 512,
      "num_id_tokens": 16,
      "hyper_dim": 128,
      "lora_rank": 8,
      "has_base_lora": false
    }
  },
  "3h": {
    "class_type": "HyperLoRALoader",
    "inputs": {
      "config": [
        "2",
        0
      ],
      "model": "sdxl_hyper_id_lora_v1_fidelity",
      "dtype": "fp16"
    }
  },
  "17": {
    "class_type": "LoadImage",
    "inputs": {
      "image": "reference.png"
    }
  },
  "4h": {
    "class_type": "HyperLoRAFaceAttr",
    "inputs": {
      "hyper_lora": [
        "3h",
        0
      ],
      "images": [
        "17",
        0
      ]
    }
  },
  "7h": {
    "class_type": "HyperLoRAIDCond",
    "inputs": {
      "hyper_lora": [
        "3h",
        0
      ],
      "images": [
        "17",
        0
      ],
      "face_attr": [
        "4h",
        0
      ],
      "grayscale": false,
      "remove_background": true
    }
  },
  "8h": {
    "class_type": "HyperLoRAGenerateIDLoRA",
    "inputs": {
      "hyper_lora": [
        "3h",
        0
      ],
      "id_cond": [
        "7h",
        0
      ]
    }
  },
  "9h": {
    "class_type": "HyperLoRAApplyLoRA",
    "inputs": {
      "model": [
        "10",
        0
      ],
      "lora": [
        "8h",
        0
      ],
      "weight": 0.8
    }
  },
  "30": {
    "class_type": "InstantIDModelLoader",
    "inputs": {
      "instantid_file": "ip-adapter.bin"
    }
  },
  "31": {
    "class_type": "InstantIDFaceAnalysis",
    "inputs": {
      "provider": "CPU"
    }
  },
  "32": {
    "class_type": "ControlNetLoader",
    "inputs": {
      "control_net_name": "instantid_controlnet.safetensors"
    }
  },
  "33": {
    "class_type": "ApplyInstantID",
    "inputs": {
      "instantid": [
        "30",
        0
      ],
      "insightface": [
        "31",
        0
      ],
      "control_net": [
        "32",
        0
      ],
      "image": [
        "17",
        0
      ],
      "model": [
        "9h",
        0
      ],
      "positive": [
        "4",
        0
      ],
      "negative": [
        "5",
        0
      ],
      "weight": 0.8,
      "weight_faceid": 0.8,
      "weight_cnet": 0.8,
      "start_at": 0.0,
      "end_at": 1.0
    }
  },
  "40": {
    "class_type": "UltralyticsDetectorProvider",
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    }
  },
  "7": {
    "class_type": "KSampler",
    "inputs": {
      "seed": 0,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "dpmpp_2m_sde",
      "scheduler": "karras",
      "denoise": 0.45,
      "model": [
        "33",
        0
      ],
      "positive": [
        "33",
        1
      ],
      "negative": [
        "33",
        2
      ],
      "latent_image": [
        "24",
        0
      ]
    }
  },
  "8": {
    "class_type": "VAEDecode",
    "inputs": {
      "samples": [
        "7",
        0
      ],
      "vae": [
        "3",
        0
      ]
    }
  },
  "50": {
    "class_type": "FaceDetailer",
    "inputs": {
      "image": [
        "8",
        0
      ],
      "model": [
        "33",
        0
      ],
      "clip": [
        "15",
        0
      ],
      "vae": [
        "3",
        0
      ],
      "positive": [
        "33",
        1
      ],
      "negative": [
        "33",
        2
      ],
      "bbox_detector": [
        "40",
        0
      ],
      "sam_model_opt": null,
      "segm_detector_opt": null,
      "detailer_hook": null,
      "scheduler_func_opt": null,
      "guide_size": 896,
      "guide_size_for": "bbox",
      "max_size": 1024,
      "seed": 812345678,
      "steps": 20,
      "cfg": 7.0,
      "sampler_name": "dpmpp_2m",
      "scheduler": "karras",
      "denoise": 0.5,
      "feather": 0.2,
      "noise_mask": false,
      "force_inpaint": false,
      "bbox_threshold": 0.5,
      "bbox_dilation": 0,
      "bbox_crop_factor": 3.0,
      "sam_detection_hint": "center-1",
      "sam_dilation": 15,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7,
      "sam_mask_hint_use_negative": "Small",
      "drop_size": 10,
      "wildcard": "",
      "cycle": 1,
      "detection_hint_min_size": 256,
      "detection_hint_max_size": 1024,
      "base_size": 896
    }
  },
  "9": {
    "class_type": "SaveImage",
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "50",
        0
      ]
    }
  }
}